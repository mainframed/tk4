              Large System Instruction Timings

                   By Richard N. Hevener

When I first began working with large IBM computers back in the
mid-1970's, one of my colleagues had a treasured card listing
instruction timings for an ancient RCA machine, not one of ours.
But we still sometimes used the card as a guide to coding because
IBM would never reveal how long instructions took to execute on
their machines.  The situation has become much more complex since
then, depending not just on fundamentals like storage access and
cache size but also on esoteric matters like look-ahead for
instruction set-up.  But I have always felt that some useful
measurement should be possible, and over the years as we acquired
new machines, I performed some crude analysis just to see, for
example, if it took longer to perform computations with LA (Load
Address) than with AR (Add Register) (generally not), if addition
were faster than subtraction (no), if multiplication were faster
than division (emphatically yes), or if fixed-point arithmetic were
faster than floating-point (only sometimes).

Late in 1996, I decided to try to obtain much more complete results
by testing in a controlled environment every general and
floating-point instruction I could find that didn't involve storage
access.  This article presents the results of that research.  I
discovered several items that surprised me and also confirmed many
others that I had long suspected.

I believe my results will be of intrinsic interest and value to
old-dog Assembler programmers.  Those who program in third generation
languages such as COBOL, C, PL/I, and FORTRAN may find the results
quite useful as well, because statements in those languages translate
fairly directly into machine code.  Indeed, all these languages
provide a mechanism for obtaining Assembly language listings of
programs if one is in doubt about exactly what machine code is
generated.  Armed with a knowledge of relative timings, one can
frequently enhance execution speed of key programs and subroutines,
both old and new.  It certainly doesn't hurt to have a general idea
of timings in mind for any sort of programming.

My reason for excluding instructions that access storage is the
great variability involved in memory retrieval under different
conditions.  I believe my results may safely be used as an
indication of relative execution speed for all types of
instructions, however, as long as one keeps in mind that there are
many variables.  Register-to-register instructions, on the other
hand, exhibit little fluctuation in the test processes explained
below, with one exception:  certain instructions apparently take
twice as long to execute under peculiar circumstances, depending on
load module size and alignment of instructions within the module
(position of the stars?).  Indeed, I will give an example of
programs that can be made to consume 50% more CPU time by adding or
deleting a single unused byte at the end.  To me, this behavior is
as fascinating as an exotic zoo beast.

My work was performed on an IBM 9672-R53 Parallel Enterprise Server
running MVS/ESA 4.2.2 and using IBM's High Level Assembler 1.2.0.  I
obtained comparable results under VM/ESA 1.2.0 using Assembler H
2.1.0.  Under both operating systems, in all cases I built and
executed load modules, as opposed to loading and executing object
modules.

My method for measuring instruction timings is to record the value
of the CPU timer both before and after a loop involving a single
RR-Format instruction (or other instruction not involving storage
access).  Subtracting these timer values and dividing by the number
of loop iterations reveals how long a single iteration takes on
average, say T.  Now the process of looping itself involves a
certain amount of overhead, so it is necessary to determine the
overhead value first and subtract it from T to get the actual
execution time for the instruction being measured.  Experimentally,
there is some variation from one run to another, but the results are
surprisingly uniform.  In my experience 50,000 loop iterations is
sufficient to stabilize measurements fairly well.  One run of this
many iterations, including determining the overhead for the loop and
testing all 68 instructions, one after another, takes less than one
second of CPU time, including assembly and link.  See Figure 1 for
the general layout of the program.

I must digress for a moment to explain the exotic anomaly mentioned
above, because we want to eliminate it as an effect in measuring
comparison timings.  It arises as follows:  Whenever the target of a
BCT loop is a four byte instruction, the loop takes one cycle
(approx. 12 nanoseconds on our machine) longer to execute if and
only if one of the following two situations occurs:  EITHER 1) the
load module length ends in 0, and the loop target is aligned on
hexadecimal E in the program listing; OR 2) the load module length
ends in 8, and the loop target is aligned on hexadecimal 6 in the
program listing.  Of course, all load modules have length ending in
either 0 or 8 and all instructions are aligned on even-numbered
bytes, so this behavior occurs on average for 1/8 of all BCT loops
whose target is a four byte instruction, at least in the cases I
have studied.

To keep the anomaly from occurring on the overhead loop itself, I
include the instruction CNOP 0,4 shortly before it.  This guarantees
that the four byte BCT instruction, its own target, will be aligned
on a full word, hence on neither 6 nor E.  I also include the same
CNOP instruction before each of the test loops for four byte
instructions: NOP, DXR, IPM, LA, and each of the eight shift
instructions.  This prevents an abnormal effect on execution times.

As I did not want to introduce variations due to storage access, I
coded several instructions so only registers, not storage, would be
accessed.  For example, all of my branching tests involve register 0
to suppress the branch.  The only test I ran of BC uses NOP to
forestall branching.  I also initialize floating-point registers
appropriately to prevent exponent overflow and general and
floating-point registers to prevent divide exceptions.

The results of my testing are presented in Figure 2.  This figure is
not a "picture" of output from the program in figure 1, but it is
derived directly from that output.  (Picoseconds are converted to
nanoseconds, for example.)  One key point that my research revealed
is that the timings, while they vary a little from run to run
(generally less than 5%), do not depend on the actual contents of
registers or on variables such as how many bits are shifted.

I would like to mention several points specifically.  In this
summary "float" means either single or double precision floating
point, which are equivalent in execution speed.  Addition and
subtraction are equivalent in all cases.  For addition, float takes
5 times as long as fixed.  For multiplication and division, float is
actually somewhat faster than fixed.  Fixed-point multiplication
takes 7 times as long as addition.  Floating-point multiplication
and addition are equivalent.  For both fixed and float, division
consumes about 3.5 times as much CPU as multiplication.  However,
extended precision float addition, takes twice as long as normal
float, multiplication takes about 3 times as long, and division is a
real killer, taking 13 times as long as normal float division, which
itself is no speed demon!  In fact, at 235 cycles DXR is by far the
worst performing instruction, taking about 10 times as long as its
nearest competitor, DR.  Despite the admonitions in the IBM
Principles of Operation manual, BALR and BASR perform equally well,
as does BR versus all other BCR instructions.  One surprise to me
was that while LR, LTR, and LDR are equivalent, LTDR takes 5 times
as long as the others.  I have touched above on only a few of the
points one may glean from Figure 2.  I encourage you to examine this
figure closely and perform additional tests if you are either
curious or suspicious.

In Figure 3 I provide source code for a program that illustrates the
BCT anomaly I have spoken of.  Alignment of the key instruction in
one of the loops will be on 6 and in the other on E.  Whenever the
test instruction is four bytes long, you should see a difference of
about one cycle in execution time.  LA, for example, will appear to
take about 12 nanoseconds in one loop and about 24 in the other,
after subtracting 24 nanoseconds for loop overhead.  The same strong
effect is observable for NOP or any of the eight shift instructions.
I happened on this behavior purely by accident, and it took me quite
a while to pin down the exact conditions when it occurs (explained
above).  It is possible, of course, that the looping itself is the
source of the abnormality rather than the instruction being executed
in the loop, but the additional CPU time consumed is very real.
Whatever the reason for it, to the best of my knowledge the
phenomenon occurs only under the conditions I have stated.

Figure 4 gives a program that clearly illustrates the additional CPU
time used as a result of the anomaly.  The idea is to align the BCT
instruction on E and make the length of the program an exact odd
multiple of 8, whence the load module length will also end in 8.
Call this program P1.  Observe that P1 does not satisfy either of
conditions 1) or 2) above.  Now add a byte at the end of P1 to
obtain P2, thereby increasing the load module length so it ends in
0.  Condition 1) is now fulfilled, and CPU time for P2 vs. P1 will
increase by 50%.  Add 7 more bytes to P2 to get P3, with module
length still ending in 0.  CPU time won't change much, because
condition 1) is still met.  Now add just one more byte to P3,
obtaining P4.  Its module length will end in 8 again, like P1, so
CPU time will drop back.  Thus, we have one program where adding an
unused byte increases CPU time by 50% (P1 vs. P2) and another where
deleting an unused byte has the same effect (P4 vs. P3).  By
increasing the number of loop iterations, for example, you can cause
P1 and P4 to take 2 minutes of CPU and P2 and P3 to take 3 minutes!
This amazes me.

I have been unable to get IBM to address this peculiar behavior at
all, though they have given me the distinct impression that they are
aware of it, without precisely admitting this.  The behavior appears
to be independent of operating system.  I suspect but am not certain
that the anomaly has been present for many generations of IBM
equipment.

As a final disclaimer, I should perhaps state that results may vary
in your environment.  I would be very interested, though, in hearing
of any marked differences in relative execution speed of
instructions on either alternative IBM equipment or on compatible
machines (Hitachi and Amdahl).  I am particularly interested, too,
in whether the abnormality I have dwelt on is manifested on other
machines.

I hope that I have shed some much needed light on instruction
timings in the mainframe world.  As we have seen, there are some
arcane phenomena present, but by and large the timings for RR-Format
instructions are remarkably stable and can be used to make informed
coding decisions.

Figure 1:  Key Elements of Main Timing Program
See TIMINGSP member.

Figure 2:  Approximate Instruction Timings in Nanoseconds

Overhead  24
AR        12
ALR       12
AXR      120
ADR       60
AER       60
AWR       60
AUR       60
NR        12
BALR      24
BASR      24
BASSM     24
BSM       24
BR        12
BNOR      12
NOP       12
BCTR      12
CR        12
CDR       60
CER       60
CLR       12
DR       272
DXR     2820
DDR      216
DER      216
XR        12
HDR       60
HER       60
IPM      252
LR        12
LDR       12
LER       12
LA        12
LTR       12
LTDR      60
LTER      60
LCR       12
LCDR      60
LCER      60
LNR       12
LNDR      60
LNER      60
LPR       12
LPDR      60
LPER      60
LRDR      60
LRER      60
MR        84
MXR      168
MXDR      72
MDR       60
MER       60
OR        12
SPM      108
SLDA      12
SLDL      12
SLA       12
SLL       12
SRDA      12
SRDL      12
SRA       12
SRL       12
SR        12
SLR       12
SXR      120
SDR       60
SER       60
SWR       60
SUR       60

Figure 3:  Key Elements of Program to Illustrate Anomaly

* SAME AS PROGRAM IN FIGURE 1 DOWN TO SAMPLE TESTS.
         MVC   LINE+1(5),=CL5'LA 1'
         CNOP  2,8            ALIGN NEXT LA ON 6 OR E
         BAS   R10,PRELOOP
LP       LA    R2,0
         BCT   R4,LP
         BAS   R10,PSTLOOP
         MVC   LINE+1(5),=CL5'LA 2'
         NOPR  0              ALIGN NEXT LA ON E OR 6
         BAS   R10,PRELOOP
LP1      LA    R2,0
         BCT   R4,LP1
         BAS   R10,PSTLOOP
* REST OF PROGRAM SAME AS FIGURE 1.

Figure 4:  Base Program to Illustrate How Tiny Changes Can Greatly
           Affect CPU Time

P1       CSECT
* INSERT STANDARD LINKAGE.
         L     R4,NLP         NUMBER OF TIMES TO LOOP
         CNOP  6,8            ALIGN BCT ON 6 OR E
* INSERT TWO 'NOP 0' INSTR. IF NEC. TO ALIGN BCT ON E.
B        BCT   R4,B
* INSERT STANDARD RETURN.
NLP      DC    F'1E8'         TAKES LESS THAN 5 SEC.
* ALTER NEXT INSTR. IF NEC. SO PGM. LEN. IS AN EXACT ODD MULTIPLE 0F 8.
* FURTHER MODIFY IT TO OBTAIN OTHER PGMS. AS EXPLAINED IN THE TEXT.
         DS    4X
         END   P1

Brief Biography

NaSPA member Richard Hevener works as a Senior Systems Programmer at
the University of South Carolina, Division of Libraries and
Information Systems.  He deals with both MVS and VM and has
programmed in a wide variety of computer languages for over 20
years.  He also has a Ph.D. in mathematics from Princeton and taught
for a number of years at the university level.  His e-mail address
is rick.hevener@sc.edu.

